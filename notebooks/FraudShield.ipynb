{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77b5bc5",
   "metadata": {},
   "source": [
    "# FraudShield: Fraud Detection with >95% Accuracy\n",
    "\n",
    "**FraudShield** is a fraud detection pipeline targeting >95% accuracy, tailored for your dataset with columns: `Transaction_ID`, `User_ID`, `Transaction_Amount`, `Transaction_Type`, `Time_of_Transaction`, `Device_Used`, `Location`, `Previous_Fraudulent_Transactions`, `Account_Age`, `Number_of_Transactions_Last_24H`, `Payment_Method`, and `Fraudulent`. This notebook:\n",
    "- Loads **Fraud Detection Dataset.csv** with encoding detection.\n",
    "- Applies feature engineering and trains a RandomForestClassifier with SMOTE.\n",
    "- Creates a Streamlit app for predictions.\n",
    "\n",
    "## Features\n",
    "- Handles missing values with imputation.\n",
    "- Interactive Streamlit dashboard with visualizations.\n",
    "- Downloads predictions and logs.\n",
    "\n",
    "## Instructions\n",
    "1. Ensure **Fraud Detection Dataset.csv** is at `C:\\Users\\heave\\Downloads\\Bron_Projects\\%Pro\\FraudShield\\data\\`.\n",
    "2. Run all cells to set up and train.\n",
    "3. Run the Streamlit app from the final cell.\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- Jupyter Notebook\n",
    "- Install dependencies: `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03518e",
   "metadata": {},
   "source": [
    "Setup Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b9429e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directories created: model_artifacts, predictions, scripts, notebooks, data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('model_artifacts', exist_ok=True)\n",
    "os.makedirs('predictions', exist_ok=True)\n",
    "os.makedirs('scripts', exist_ok=True)\n",
    "os.makedirs('notebooks', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print('Project directories created: model_artifacts, predictions, scripts, notebooks, data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30937a",
   "metadata": {},
   "source": [
    "Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f4c8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heave\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected file encoding: ascii\n",
      "Dataset loaded successfully. Shape: (51000, 12)\n",
      "Test dataset saved to data\\sample_transactions.csv with 10024 samples\n",
      "Training dataset has 50119 samples, 2467 frauds (4.92% fraud rate)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chardet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set the file path\n",
    "file_path = r\"C:\\Users\\heave\\Downloads\\Bron_Projects\\%Pro\\FraudShield\\Fraud Detection Dataset.csv\"\n",
    "\n",
    "# Detect encoding\n",
    "with open(file_path, \"rb\") as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"Detected file encoding: {encoding}\")\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding=encoding)\n",
    "    print(f'Dataset loaded successfully. Shape: {df.shape}')\n",
    "except FileNotFoundError:\n",
    "    print('Error: Fraud Detection Dataset.csv not found in the specified path. Please check and try again.')\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Feature engineering with handling for missing values\n",
    "try:\n",
    "    df['Transaction_Frequency'] = df.groupby('User_ID')['Transaction_ID'].transform('count') / df['Account_Age'].replace('', 1).astype(float)\n",
    "    df['Amount_ZScore'] = (df['Transaction_Amount'] - df.groupby('User_ID')['Transaction_Amount'].transform('mean')) / df.groupby('User_ID')['Transaction_Amount'].transform('std').fillna(1)\n",
    "    df['Is_Night_Transaction'] = df['Time_of_Transaction'].replace('', 0).astype(float).apply(lambda x: 1 if 0 <= x <= 6 else 0)\n",
    "    df['Transaction_Velocity'] = df['Number_of_Transactions_Last_24H'] / (df['Account_Age'].replace('', 1).astype(float) / 30).clip(lower=1)\n",
    "    df['Location_Anomaly'] = df.groupby('User_ID')['Location'].transform(lambda x: 1 if x.nunique() > 2 else 0)\n",
    "    df['Transaction_Acceleration'] = df['Number_of_Transactions_Last_24H'] / df.groupby('User_ID')['Number_of_Transactions_Last_24H'].transform('mean').clip(lower=1)\n",
    "    df['Device_Anomaly'] = df.groupby('User_ID')['Device_Used'].transform(lambda x: 1 if x.nunique() > 2 else 0)\n",
    "except Exception as e:\n",
    "    print(f\"Error during feature engineering: {e}\")\n",
    "    raise\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save test dataset (20% sample, without target column)\n",
    "try:\n",
    "    test_df = df.sample(frac=0.2, random_state=42).copy()\n",
    "    if 'Fraudulent' in test_df.columns:\n",
    "        test_df = test_df.drop('Fraudulent', axis=1)\n",
    "\n",
    "    output_path = os.path.join(\"data\", \"sample_transactions.csv\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    test_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f'Test dataset saved to {output_path} with {len(test_df)} samples')\n",
    "\n",
    "    if 'Fraudulent' in df.columns:\n",
    "        fraud_rate = df[\"Fraudulent\"].mean() * 100\n",
    "        print(f'Training dataset has {len(df)} samples, {df[\"Fraudulent\"].sum()} frauds ({fraud_rate:.2f}% fraud rate)')\n",
    "    else:\n",
    "        print(\"Warning: 'Fraudulent' column not found. Training will be skipped.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving test dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6bd633",
   "metadata": {},
   "source": [
    "Train and Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from joblib import dump\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "if 'Fraudulent' not in df.columns:\n",
    "    print(\"Error: 'Fraudulent' column is missing. Please ensure the dataset includes the target column and rerun.\")\n",
    "else:\n",
    "    # Preprocess data\n",
    "    categorical_cols = ['Transaction_Type', 'Device_Used', 'Location', 'Payment_Method']\n",
    "    numerical_cols = ['User_ID', 'Transaction_Amount', 'Time_of_Transaction', 'Previous_Fraudulent_Transactions', \n",
    "                      'Account_Age', 'Number_of_Transactions_Last_24H', 'Transaction_Frequency', 'Amount_ZScore', \n",
    "                      'Is_Night_Transaction', 'Transaction_Velocity', 'Location_Anomaly', 'Transaction_Acceleration', 'Device_Anomaly']\n",
    "\n",
    "    # Imputation for missing values\n",
    "    df = df.fillna({\n",
    "        'Transaction_Type': 'Unknown',\n",
    "        'Device_Used': 'Unknown',\n",
    "        'Location': 'Unknown',\n",
    "        'Payment_Method': 'Unknown',\n",
    "        'Time_of_Transaction': 0,\n",
    "        'Account_Age': 1\n",
    "    })\n",
    "    precomputed_means = df[numerical_cols].mean()\n",
    "    precomputed_modes = df[categorical_cols].mode().iloc[0]\n",
    "    df = df.fillna(precomputed_means).fillna(precomputed_modes)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            encoders[col] = LabelEncoder().fit(df[col].astype(str))\n",
    "            df[col] = encoders[col].transform(df[col].astype(str))\n",
    "\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "    # Features and target\n",
    "    feature_names = [col for col in numerical_cols + categorical_cols if col in df.columns]\n",
    "    X = df[feature_names]\n",
    "    y = df['Fraudulent']\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'class_weight': ['balanced', None]\n",
    "    }\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-Score: {f1:.4f}')\n",
    "    print(f'ROC-AUC: {roc_auc:.4f}')\n",
    "\n",
    "    # Visualizations\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(recall_vals, precision_vals, label='Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "\n",
    "    # Save artifacts\n",
    "    dump(best_model, 'model_artifacts/ensemble_model.joblib')\n",
    "    dump(feature_names, 'model_artifacts/feature_names.joblib')\n",
    "    dump(scaler, 'model_artifacts/scaler.joblib')\n",
    "    dump(precomputed_means, 'model_artifacts/precomputed_means.joblib')\n",
    "    dump(precomputed_modes, 'model_artifacts/precomputed_modes.joblib')\n",
    "    for col, encoder in encoders.items():\n",
    "        dump(encoder, f'model_artifacts/encoder_{col}.joblib')\n",
    "\n",
    "    metadata = {\n",
    "        'Model': 'RandomForestClassifier',\n",
    "        'Version': '4.0',\n",
    "        'Training_Date': '2025-08-05 17:15 GMT',\n",
    "        'Accuracy': float(accuracy),\n",
    "        'Best_Parameters': grid_search.best_params_,\n",
    "        'Feature_Importance': feature_importance.to_dict(orient='records')\n",
    "    }\n",
    "    with open('model_artifacts/model_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    print('Model and artifacts saved to model_artifacts/')\n",
    "    if accuracy < 0.95:\n",
    "        print('Warning: Accuracy below 95%. Consider expanding param_grid or adding features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52db4d",
   "metadata": {},
   "source": [
    "CREATE PREDICTION LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ef74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from joblib import load\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='fraud_detection_predict.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "MODEL_DIR = 'model_artifacts'\n",
    "PREDICTIONS_DIR = 'predictions'\n",
    "\n",
    "def load_data(file=None):\n",
    "    import chardet\n",
    "    import pandas as pd\n",
    "    if file is not None:\n",
    "        try:\n",
    "            if isinstance(file, str):\n",
    "                with open(file, \"rb\") as f:\n",
    "                    result = chardet.detect(f.read())\n",
    "                    encoding = result['encoding']\n",
    "                if file.endswith('.csv'):\n",
    "                    df = pd.read_csv(file, encoding=encoding)\n",
    "                elif file.endswith('.json'):\n",
    "                    df = pd.read_json(file)\n",
    "                elif file.endswith(('.xlsx', '.xls')):\n",
    "                    df = pd.read_excel(file)\n",
    "                else:\n",
    "                    raise ValueError('Unsupported file format. Use CSV, JSON, or Excel.')\n",
    "            else:\n",
    "                if file.name.endswith('.csv'):\n",
    "                    with open(file.name, \"rb\") as f:\n",
    "                        result = chardet.detect(f.read())\n",
    "                        encoding = result['encoding']\n",
    "                    df = pd.read_csv(file, encoding=encoding)\n",
    "                elif file.name.endswith('.json'):\n",
    "                    df = pd.read_json(file)\n",
    "                elif file.name.endswith(('.xlsx', '.xls')):\n",
    "                    df = pd.read_excel(file)\n",
    "                else:\n",
    "                    raise ValueError('Unsupported file format. Use CSV, JSON, or Excel.')\n",
    "            logging.info(f'Dataset loaded successfully from {file}. Shape: {df.shape}')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error loading file {file}: {str(e)}')\n",
    "            raise\n",
    "    logging.info('Loading sample dataset.')\n",
    "    df = pd.read_csv('data/sample_transactions.csv')\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, encoders, feature_names, scaler, precomputed_means, precomputed_modes):\n",
    "    logging.info('Starting data preprocessing...')\n",
    "    try:\n",
    "        df['Transaction_Frequency'] = df.groupby('User_ID')['Transaction_ID'].transform('count') / df['Account_Age'].replace('', 1).astype(float)\n",
    "        df['Amount_ZScore'] = (df['Transaction_Amount'] - df.groupby('User_ID')['Transaction_Amount'].transform('mean')) / df.groupby('User_ID')['Transaction_Amount'].transform('std').fillna(1)\n",
    "        df['Is_Night_Transaction'] = df['Time_of_Transaction'].replace('', 0).astype(float).apply(lambda x: 1 if 0 <= x <= 6 else 0)\n",
    "        df['Transaction_Velocity'] = df['Number_of_Transactions_Last_24H'] / (df['Account_Age'].replace('', 1).astype(float) / 30).clip(lower=1)\n",
    "        df['Location_Anomaly'] = df.groupby('User_ID')['Location'].transform(lambda x: 1 if x.nunique() > 2 else 0)\n",
    "        df['Transaction_Acceleration'] = df['Number_of_Transactions_Last_24H'] / df.groupby('User_ID')['Number_of_Transactions_Last_24H'].transform('mean').clip(lower=1)\n",
    "        df['Device_Anomaly'] = df.groupby('User_ID')['Device_Used'].transform(lambda x: 1 if x.nunique() > 2 else 0)\n",
    "\n",
    "        df = df.fillna({\n",
    "            'Transaction_Type': 'Unknown',\n",
    "            'Device_Used': 'Unknown',\n",
    "            'Location': 'Unknown',\n",
    "            'Payment_Method': 'Unknown',\n",
    "            'Time_of_Transaction': 0,\n",
    "            'Account_Age': 1\n",
    "        })\n",
    "        df = df.fillna(precomputed_means).fillna(precomputed_modes)\n",
    "        logging.info('Missing values filled with precomputed means and modes.')\n",
    "        \n",
    "        for col in encoders:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    df[col] = encoders[col].transform(df[col].astype(str))\n",
    "                except ValueError:\n",
    "                    unknown_count = sum(~df[col].astype(str).isin(encoders[col].classes_))\n",
    "                    logging.warning(f'{unknown_count} unknown categories in {col}. Using default encoding.')\n",
    "                    df[col] = df[col].astype(str).map(lambda x: x if x in encoders[col].classes_ else encoders[col].classes_[0])\n",
    "                    df[col] = encoders[col].transform(df[col])\n",
    "        \n",
    "        numerical_cols = [col for col in feature_names if col in df.columns and df[col].dtype in ['int64', 'float64']]\n",
    "        if numerical_cols:\n",
    "            df[numerical_cols] = scaler.transform(df[numerical_cols])\n",
    "            logging.info(f'Scaled numerical columns: {numerical_cols}')\n",
    "        \n",
    "        for col in feature_names:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "                logging.warning(f'Added missing feature {col} with default value 0.')\n",
    "        \n",
    "        if 'Transaction_ID' in df.columns:\n",
    "            transaction_ids = df['Transaction_ID']\n",
    "            df = df.drop('Transaction_ID', axis=1)\n",
    "        else:\n",
    "            transaction_ids = [f'T{i}' for i in range(len(df))]\n",
    "        \n",
    "        df = df[feature_names]\n",
    "        logging.info('Data preprocessing completed.')\n",
    "        return df, transaction_ids\n",
    "    except Exception as e:\n",
    "        logging.error(f'Data preprocessing failed: {str(e)}')\n",
    "        raise\n",
    "\n",
    "def predict_fraud(df=None):\n",
    "    try:\n",
    "        required_files = [\n",
    "            'ensemble_model.joblib', 'feature_names.joblib', 'scaler.joblib',\n",
    "            'precomputed_means.joblib', 'precomputed_modes.joblib',\n",
    "            'encoder_Transaction_Type.joblib', 'encoder_Device_Used.joblib',\n",
    "            'encoder_Location.joblib', 'encoder_Payment_Method.joblib'\n",
    "        ]\n",
    "        for file_name in required_files:\n",
    "            if not os.path.exists(os.path.join(MODEL_DIR, file_name)):\n",
    "                raise FileNotFoundError(f'Required file {file_name} not found in {MODEL_DIR}.')\n",
    "        \n",
    "        model = load(os.path.join(MODEL_DIR, 'ensemble_model.joblib'))\n",
    "        feature_names = load(os.path.join(MODEL_DIR, 'feature_names.joblib'))\n",
    "        scaler = load(os.path.join(MODEL_DIR, 'scaler.joblib'))\n",
    "        precomputed_means = load(os.path.join(MODEL_DIR, 'precomputed_means.joblib'))\n",
    "        precomputed_modes = load(os.path.join(MODEL_DIR, 'precomputed_modes.joblib'))\n",
    "        encoders = {\n",
    "            'Transaction_Type': load(os.path.join(MODEL_DIR, 'encoder_Transaction_Type.joblib')),\n",
    "            'Device_Used': load(os.path.join(MODEL_DIR, 'encoder_Device_Used.joblib')),\n",
    "            'Location': load(os.path.join(MODEL_DIR, 'encoder_Location.joblib')),\n",
    "            'Payment_Method': load(os.path.join(MODEL_DIR, 'encoder_Payment_Method.joblib'))\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(MODEL_DIR, 'model_metadata.json')\n",
    "        metadata = {'Model': 'Unknown', 'Version': 'Unknown', 'Training_Date': 'Unknown', 'Accuracy': 0.0}\n",
    "        if os.path.exists(metadata_path):\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "        \n",
    "        if df is None:\n",
    "            df = load_data()\n",
    "        \n",
    "        X, transaction_ids = preprocess_data(df, encoders, feature_names, scaler, precomputed_means, precomputed_modes)\n",
    "        \n",
    "        predictions = model.predict(X)\n",
    "        fraud_probabilities = model.predict_proba(X)[:, 1]\n",
    "        logging.info(f'Predicted {sum(predictions)} fraudulent transactions out of {len(predictions)}.')\n",
    "        \n",
    "        results = pd.DataFrame({\n",
    "            'Transaction_ID': transaction_ids,\n",
    "            'Fraud_Prediction': ['Fraudulent' if pred == 1 else 'Legitimate' for pred in predictions],\n",
    "            'Fraud_Probability': fraud_probabilities\n",
    "        })\n",
    "        results['Fraud_Probability'] = results['Fraud_Probability'].round(4)\n",
    "        \n",
    "        os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
    "        json_path = os.path.join(PREDICTIONS_DIR, f'predictions_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json')\n",
    "        excel_path = os.path.join(PREDICTIONS_DIR, f'predictions_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        results.to_json(json_path, orient='records', indent=4)\n",
    "        results.to_excel(excel_path, index=False)\n",
    "        \n",
    "        return results, json_path, excel_path, metadata\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f'Model or artifact files not found: {str(e)}')\n",
    "        print(f'Error: {str(e)}. Run the training cell to generate artifacts.')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f'Fraud prediction failed: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# Test prediction\n",
    "results, json_path, excel_path, metadata = predict_fraud('data/sample_transactions.csv')\n",
    "print('Prediction Results:')\n",
    "display(results.head())\n",
    "print(f'Results saved to {json_path} and {excel_path}')\n",
    "print('Metadata:', metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8286a3b",
   "metadata": {},
   "source": [
    "CREATE STREAMLIT APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from joblib import load\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='fraud_detection_predict.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "MODEL_DIR = os.getenv('MODEL_DIR', 'model_artifacts')\n",
    "PREDICTIONS_DIR = os.getenv('PREDICTIONS_DIR', 'predictions')\n",
    "RETENTION_DAYS = 7\n",
    "\n",
    "st.set_page_config(page_title='FraudShield Dashboard', layout='wide', page_icon='ðŸ›¡ï¸')\n",
    "\n",
    "def cleanup_old_files(directory, retention_days=RETENTION_DAYS):\n",
    "    try:\n",
    "        now = datetime.now()\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "                if now - file_mtime > timedelta(days=retention_days):\n",
    "                    os.remove(file_path)\n",
    "                    logging.info(f'Deleted old file: {file_path}')\n",
    "        log_file = 'fraud_detection_predict.log'\n",
    "        if os.path.exists(log_file):\n",
    "            log_mtime = datetime.fromtimestamp(os.path.getmtime(log_file))\n",
    "            if now - log_mtime > timedelta(days=retention_days):\n",
    "                with open(log_file, 'w'):\n",
    "                    pass\n",
    "                logging.info('Truncated old log file.')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Failed to clean up old files: {str(e)}')\n",
    "        st.warning(f'Could not clean up old files: {str(e)}')\n",
    "\n",
    "def load_data(file=None):\n",
    "    import chardet\n",
    "    if file is not None:\n",
    "        try:\n",
    "            if isinstance(file, str):\n",
    "                with open(file, \"rb\") as f:\n",
    "                    result = chardet.detect(f.read())\n",
    "                    encoding = result['encoding']\n",
    "                if file.endswith('.csv'):\n",
    "                    df = pd.read_csv(file, encoding=encoding)\n",
    "                elif file.endswith('.json'):\n",
    "                    df = pd.read_json(file)\n",
    "                elif file.endswith(('.xlsx', '.xls')):\n",
    "                    df = pd.read_excel(file)\n",
    "                else:\n",
    "                    raise ValueError('Unsupported file format. Use CSV, JSON, or Excel.')\n",
    "            else:\n",
    "                if file.name.endswith('.csv'):\n",
    "                    with open(file.name, \"rb\") as f:\n",
    "                        result = chardet.detect(f.read())\n",
    "                        encoding = result['encoding']\n",
    "                    df = pd.read_csv(file, encoding=encoding)\n",
    "                elif file.name.endswith('.json'):\n",
    "                    df = pd.read_json(file)\n",
    "                elif file.name.endswith(('.xlsx', '.xls')):\n",
    "                    df = pd.read_excel(file)\n",
    "                else:\n",
    "                    raise ValueError('Unsupported file format. Use CSV, JSON, or Excel.')\n",
    "            logging.info(f'Dataset loaded successfully from {file}. Shape: {df.shape}')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error loading file {file}: {str(e)}')\n",
    "            st.error(f'Error loading file: {str(e)}')\n",
    "            return None\n",
    "    logging.info('Loading sample dataset.')\n",
    "    df = pd.read_csv('data/sample_transactions.csv')\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, encoders, feature_names, scaler, precomputed_means, precomputed_modes):\n",
    "    logging.info('Starting data preprocessing...')\n",
    "    try:\n",
    "        df['Transaction_Frequency'] = df.groupby('User_ID')['Transaction_ID'].transform('count') / df['Account_Age'].replace('', 1).astype(float)\n",
    "        df['Amount_ZScore'] = (df['Transaction_Amount'] - df.groupby('User_ID')['Transaction_Amount'].transform('mean')) / df.groupby('User_ID')['Transaction_Amount'].transform('std').fillna(1)\n",
    "        df['Is_Night_Transaction'] = df['Time_of_Transaction'].replace('', 0).astype(float).apply(lambda x: 1 if 0 <= x <= 6 else 0)\n",
    "        df['Transaction_Velocity'] = df['Number_of_Transactions_Last_24H'] / (df['Account_Age'].replace('', 1).astype(float) / 30).clip(lower=1)\n",
    "        df['Location_Anomaly'] = df.groupby('User_ID')['Location'].transform(lambda x: 1 if x.nunique() > 2 else 0)\n",
    "        df['Transaction_Acceleration'] = df['Number_of_Transactions_Last_24H'] / df.groupby('User_ID')['Number_of_Transactions_Last_24H'].transform('mean').clip(lower=1)\n",
    "        df['Device_Anomaly'] = df.groupby('User_ID')['Device_Used'].transform(lambda x: 1 if x.nunique() > 2 else 0)\n",
    "\n",
    "        df = df.fillna({\n",
    "            'Transaction_Type': 'Unknown',\n",
    "            'Device_Used': 'Unknown',\n",
    "            'Location': 'Unknown',\n",
    "            'Payment_Method': 'Unknown',\n",
    "            'Time_of_Transaction': 0,\n",
    "            'Account_Age': 1\n",
    "        })\n",
    "        df = df.fillna(precomputed_means).fillna(precomputed_modes)\n",
    "        logging.info('Missing values filled with precomputed means and modes.')\n",
    "        \n",
    "        for col in encoders:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    df[col] = encoders[col].transform(df[col].astype(str))\n",
    "                except ValueError:\n",
    "                    unknown_count = sum(~df[col].astype(str).isin(encoders[col].classes_))\n",
    "                    logging.warning(f'{unknown_count} unknown categories in {col}. Using default encoding.')\n",
    "                    df[col] = df[col].astype(str).map(lambda x: x if x in encoders[col].classes_ else encoders[col].classes_[0])\n",
    "                    df[col] = encoders[col].transform(df[col])\n",
    "        \n",
    "        numerical_cols = [col for col in feature_names if col in df.columns and df[col].dtype in ['int64', 'float64']]\n",
    "        if numerical_cols:\n",
    "            df[numerical_cols] = scaler.transform(df[numerical_cols])\n",
    "            logging.info(f'Scaled numerical columns: {numerical_cols}')\n",
    "        \n",
    "        for col in feature_names:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "                logging.warning(f'Added missing feature {col} with default value 0.')\n",
    "        \n",
    "        if 'Transaction_ID' in df.columns:\n",
    "            transaction_ids = df['Transaction_ID']\n",
    "            df = df.drop('Transaction_ID', axis=1)\n",
    "        else:\n",
    "            transaction_ids = [f'T{i}' for i in range(len(df))]\n",
    "        \n",
    "        df = df[feature_names]\n",
    "        logging.info('Data preprocessing completed.')\n",
    "        return df, transaction_ids\n",
    "    except Exception as e:\n",
    "        logging.error(f'Data preprocessing failed: {str(e)}')\n",
    "        st.error(f'Data preprocessing failed: {str(e)}')\n",
    "        return None, None\n",
    "\n",
    "def predict_fraud(df=None):\n",
    "    try:\n",
    "        required_files = [\n",
    "            'ensemble_model.joblib', 'feature_names.joblib', 'scaler.joblib',\n",
    "            'precomputed_means.joblib', 'precomputed_modes.joblib',\n",
    "            'encoder_Transaction_Type.joblib', 'encoder_Device_Used.joblib',\n",
    "            'encoder_Location.joblib', 'encoder_Payment_Method.joblib'\n",
    "        ]\n",
    "        for file_name in required_files:\n",
    "            if not os.path.exists(os.path.join(MODEL_DIR, file_name)):\n",
    "                raise FileNotFoundError(f'Required file {file_name} not found in {MODEL_DIR}.')\n",
    "        \n",
    "        model = load(os.path.join(MODEL_DIR, 'ensemble_model.joblib'))\n",
    "        feature_names = load(os.path.join(MODEL_DIR, 'feature_names.joblib'))\n",
    "        scaler = load(os.path.join(MODEL_DIR, 'scaler.joblib'))\n",
    "        precomputed_means = load(os.path.join(MODEL_DIR, 'precomputed_means.joblib'))\n",
    "        precomputed_modes = load(os.path.join(MODEL_DIR, 'precomputed_modes.joblib'))\n",
    "        encoders = {\n",
    "            'Transaction_Type': load(os.path.join(MODEL_DIR, 'encoder_Transaction_Type.joblib')),\n",
    "            'Device_Used': load(os.path.join(MODEL_DIR, 'encoder_Device_Used.joblib')),\n",
    "            'Location': load(os.path.join(MODEL_DIR, 'encoder_Location.joblib')),\n",
    "            'Payment_Method': load(os.path.join(MODEL_DIR, 'encoder_Payment_Method.joblib'))\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(MODEL_DIR, 'model_metadata.json')\n",
    "        metadata = {'Model': 'Unknown', 'Version': 'Unknown', 'Training_Date': 'Unknown', 'Accuracy': 0.0}\n",
    "        if os.path.exists(metadata_path):\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "        \n",
    "        if df is None:\n",
    "            df = load_data()\n",
    "            if df is None:\n",
    "                return None, None, None, None\n",
    "        \n",
    "        X, transaction_ids = preprocess_data(df, encoders, feature_names, scaler, precomputed_means, precomputed_modes)\n",
    "        if X is None:\n",
    "            return None, None, None, None\n",
    "        \n",
    "        predictions = model.predict(X)\n",
    "        fraud_probabilities = model.predict_proba(X)[:, 1]\n",
    "        logging.info(f'Predicted {sum(predictions)} fraudulent transactions out of {len(predictions)}.')\n",
    "        \n",
    "        results = pd.DataFrame({\n",
    "            'Transaction_ID': transaction_ids,\n",
    "            'Fraud_Prediction': ['Fraudulent' if pred == 1 else 'Legitimate' for pred in predictions],\n",
    "            'Fraud_Probability': fraud_probabilities\n",
    "        })\n",
    "        results['Fraud_Probability'] = results['Fraud_Probability'].round(4)\n",
    "        \n",
    "        os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
    "        json_path = os.path.join(PREDICTIONS_DIR, f'predictions_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json')\n",
    "        excel_path = os.path.join(PREDICTIONS_DIR, f'predictions_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        results.to_json(json_path, orient='records', indent=4)\n",
    "        results.to_excel(excel_path, index=False)\n",
    "        \n",
    "        return results, json_path, excel_path, metadata\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f'Model or artifact files not found: {str(e)}')\n",
    "        st.error(f'Error: {str(e)}. Run the training cell to generate artifacts.')\n",
    "        return None, None, None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f'Fraud prediction failed: {str(e)}')\n",
    "        st.error(f'Prediction failed: {str(e)}')\n",
    "        return None, None, None, None\n",
    "\n",
    "def main():\n",
    "    st.title('ðŸ›¡ï¸ FraudShield Dashboard')\n",
    "    st.markdown('Upload a transaction dataset (CSV, JSON, or Excel) or use the sample dataset to predict fraudulent transactions.')\n",
    "\n",
    "    cleanup_old_files(PREDICTIONS_DIR)\n",
    "    \n",
    "    st.sidebar.header('Data Input & Filters')\n",
    "    uploaded_file = st.sidebar.file_uploader('Upload Transactions', type=['csv', 'json', 'xlsx', 'xls'])\n",
    "    \n",
    "    df = load_data(uploaded_file)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    results, json_path, excel_path, metadata = predict_fraud(df)\n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    st.sidebar.header('Model Metadata')\n",
    "    st.sidebar.text(f'**Model**: {metadata.get(\"Model\", \"Unknown\")}')\n",
    "    st.sidebar.text(f'**Version**: {metadata.get(\"Version\", \"Unknown\")}')\n",
    "    st.sidebar.text(f'**Training Date**: {metadata.get(\"Training_Date\", \"Unknown\")}')\n",
    "    st.sidebar.text(f'**Accuracy**: {metadata.get(\"Accuracy\", 0.0):.4f}')\n",
    "    \n",
    "    st.header('Prediction Summary')\n",
    "    fraud_count = len(results[results['Fraud_Prediction'] == 'Fraudulent'])\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    col1.metric('Total Transactions', len(results))\n",
    "    col2.metric('Fraudulent Transactions', f'{fraud_count} ({fraud_count/len(results)*100:.2f}%)')\n",
    "    col3.metric('Average Fraud Probability', f'{results[\"Fraud_Probability\"].mean():.4f}')\n",
    "    \n",
    "    st.sidebar.subheader('Filter Results')\n",
    "    prediction_filter = st.sidebar.selectbox('Filter by Prediction', ['All', 'Fraudulent', 'Legitimate'])\n",
    "    probability_threshold = st.sidebar.slider('Fraud Probability Threshold', 0.0, 1.0, 0.5, 0.01)\n",
    "    \n",
    "    filtered_results = results.copy()\n",
    "    if prediction_filter != 'All':\n",
    "        filtered_results = filtered_results[filtered_results['Fraud_Prediction'] == prediction_filter]\n",
    "    filtered_results = filtered_results[filtered_results['Fraud_Probability'] >= probability_threshold]\n",
    "    \n",
    "    st.header('Top 5 Risky Transactions')\n",
    "    top_risky = results.sort_values(by='Fraud_Probability', ascending=False).head(5)\n",
    "    st.dataframe(top_risky, use_container_width=True)\n",
    "    \n",
    "    st.download_button(\n",
    "        label='Download JSON Results',\n",
    "        data=open(json_path, 'r').read(),\n",
    "        file_name=os.path.basename(json_path),\n",
    "        mime='application/json'\n",
    "    )\n",
    "    st.download_button(\n",
    "        label='Download Excel Results',\n",
    "        data=open(excel_path, 'rb').read(),\n",
    "        file_name=os.path.basename(excel_path),\n",
    "        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('scripts/app.py', 'w') as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print('Streamlit app saved to scripts/app.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b431d6",
   "metadata": {},
   "source": [
    "CREAT PREDICTION SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d65508",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fraud_code = '''\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='fraud_detection_predict.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "MODEL_DIR = 'model_artifacts'\n",
    "PREDICTIONS_DIR = 'predictions'\n",
    "\n",
    "def load_data(file=None):\n",
    "    if file is not None:\n",
    "        try:\n",
    "            if isinstance(file, str):\n",
    "                with open(file, \"rb\") as f:\n",
    "                    result = chardet.detect(f.read())\n",
    "                    encoding = result['encoding']\n",
    "                if file.endswith('.csv'):\n",
    "                    df = pd.read_csv(file, encoding=encoding)\n",
    "                elif file.endswith('.json'):\n",
    "                    df = pd.read_json(file)\n",
    "                elif file.endswith(('.xlsx', '.xls')):\n",
    "                    df = pd.read_excel(file)\n",
    "                else:\n",
    "                    raise ValueError('Unsupported file format. Use CSV, JSON, or Excel.')\n",
    "            else:\n",
    "                if file.name.endswith('.csv'):\n",
    "                    with open(file.name, \"rb\") as f:\n",
    "                        result = chardet.detect(f.read())\n",
    "                        encoding = result['encoding']\n",
    "                    df = pd.read_csv(file, encoding=encoding)\n",
    "                elif file.name.endswith('.json'):\n",
    "                    df = pd.read_json(file)\n",
    "                elif file.name.endswith(('.xlsx', '.xls')):\n",
    "                    df = pd.read_excel(file)\n",
    "                else:\n",
    "                    raise ValueError('Unsupported file format. Use CSV, JSON, or Excel.')\n",
    "            logging.info(f'Dataset loaded successfully from {file}. Shape: {df.shape}')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error loading file {file}: {str(e)}')\n",
    "            raise\n",
    "    logging.info('Loading sample dataset.')\n",
    "    df = pd.read_csv('data/sample_transactions.csv')\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, encoders, feature_names, scaler, precomputed_means, precomputed_modes):\n",
    "    logging.info('Starting data preprocessing...')\n",
    "    try:\n",
    "        df['Transaction_Frequency'] = df.groupby('User_ID')['Transaction_ID'].transform('count') / df['Account_Age'].replace('', 1).astype(float)\n",
    "        df['Amount_ZScore'] = (df['Transaction_Amount'] - df.groupby('User_ID')['Transaction_Amount'].transform('mean')) / df.groupby('User_ID')['Transaction_Amount'].transform('std').fillna(1)\n",
    "        df['Is_Night_Transaction'] = df['Time_of_Transaction'].replace('', 0).astype(float).apply(lambda x: 1 if 0 <= x <= 6 else 0)\n",
    "        df['Transaction_Velocity'] = df['Number_of_Transactions_Last_24H'] / (df['Account_Age'].replace('', 1).astype(float) / 30).clip(lower=1)\n",
    "        df['Location_Anomaly'] = df.groupby('User_ID')['Location'].transform(lambda x: 1 if x.nunique() > 2 else 0)\n",
    "        df['Transaction_Acceleration'] = df['Number_of_Transactions_Last_24H'] / df.groupby('User_ID')['Number_of_Transactions_Last_24H'].transform('mean').clip(lower=1)\n",
    "        df['Device_Anomaly'] = df.groupby('User_ID')['Device_Used'].transform(lambda x: 1 if x.nunique() > 2 else 0)\n",
    "\n",
    "        df = df.fillna({\n",
    "            'Transaction_Type': 'Unknown',\n",
    "            'Device_Used': 'Unknown',\n",
    "            'Location': 'Unknown',\n",
    "            'Payment_Method': 'Unknown',\n",
    "            'Time_of_Transaction': 0,\n",
    "            'Account_Age': 1\n",
    "        })\n",
    "        df = df.fillna(precomputed_means).fillna(precomputed_modes)\n",
    "        logging.info('Missing values filled with precomputed means and modes.')\n",
    "        \n",
    "        for col in encoders:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    df[col] = encoders[col].transform(df[col].astype(str))\n",
    "                except ValueError:\n",
    "                    unknown_count = sum(~df[col].astype(str).isin(encoders[col].classes_))\n",
    "                    logging.warning(f'{unknown_count} unknown categories in {col}. Using default encoding.')\n",
    "                    df[col] = df[col].astype(str).map(lambda x: x if x in encoders[col].classes_ else encoders[col].classes_[0])\n",
    "                    df[col] = encoders[col].transform(df[col])\n",
    "        \n",
    "        numerical_cols = [col for col in feature_names if col in df.columns and df[col].dtype in ['int64', 'float64']]\n",
    "        if numerical_cols:\n",
    "            df[numerical_cols] = scaler.transform(df[numerical_cols])\n",
    "            logging.info(f'Scaled numerical columns: {numerical_cols}')\n",
    "        \n",
    "        for col in feature_names:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "                logging.warning(f'Added missing feature {col} with default value 0.')\n",
    "        \n",
    "        if 'Transaction_ID' in df.columns:\n",
    "            transaction_ids = df['Transaction_ID']\n",
    "            df = df.drop('Transaction_ID', axis=1)\n",
    "        else:\n",
    "            transaction_ids = [f'T{i}' for i in range(len(df))]\n",
    "        \n",
    "        df = df[feature_names]\n",
    "        logging.info('Data preprocessing completed.')\n",
    "        return df, transaction_ids\n",
    "    except Exception as e:\n",
    "        logging.error(f'Data preprocessing failed: {str(e)}')\n",
    "        raise\n",
    "\n",
    "def predict_fraud(df=None):\n",
    "    try:\n",
    "        required_files = [\n",
    "            'ensemble_model.joblib', 'feature_names.joblib', 'scaler.joblib',\n",
    "            'precomputed_means.joblib', 'precomputed_modes.joblib',\n",
    "            'encoder_Transaction_Type.joblib', 'encoder_Device_Used.joblib',\n",
    "            'encoder_Location.joblib', 'encoder_Payment_Method.joblib'\n",
    "        ]\n",
    "        for file_name in required_files:\n",
    "            if not os.path.exists(os.path.join(MODEL_DIR, file_name)):\n",
    "                raise FileNotFoundError(f'Required file {file_name} not found in {MODEL_DIR}.')\n",
    "        \n",
    "        model = load(os.path.join(MODEL_DIR, 'ensemble_model.joblib'))\n",
    "        feature_names = load(os.path.join(MODEL_DIR, 'feature_names.joblib'))\n",
    "        scaler = load(os.path.join(MODEL_DIR, 'scaler.joblib'))\n",
    "        precomputed_means = load(os.path.join(MODEL_DIR, 'precomputed_means.joblib'))\n",
    "        precomputed_modes = load(os.path.join(MODEL_DIR, 'precomputed_modes.joblib'))\n",
    "        encoders = {\n",
    "            'Transaction_Type': load(os.path.join(MODEL_DIR, 'encoder_Transaction_Type.joblib')),\n",
    "            'Device_Used': load(os.path.join(MODEL_DIR, 'encoder_Device_Used.joblib')),\n",
    "            'Location': load(os.path.join(MODEL_DIR, 'encoder_Location.joblib')),\n",
    "            'Payment_Method': load(os.path.join(MODEL_DIR, 'encoder_Payment_Method.joblib'))\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(MODEL_DIR, 'model_metadata.json')\n",
    "        metadata = {'Model': 'Unknown', 'Version': 'Unknown', 'Training_Date': 'Unknown', 'Accuracy': 0.0}\n",
    "        if os.path.exists(metadata_path):\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "        \n",
    "        if df is None:\n",
    "            df = load_data()\n",
    "        \n",
    "        X, transaction_ids = preprocess_data(df, encoders, feature_names, scaler, precomputed_means, precomputed_modes)\n",
    "        \n",
    "        predictions = model.predict(X)\n",
    "        fraud_probabilities = model.predict_proba(X)[:, 1]\n",
    "        logging.info(f'Predicted {sum(predictions)} fraudulent transactions out of {len(predictions)}.')\n",
    "        \n",
    "        results = pd.DataFrame({\n",
    "            'Transaction_ID': transaction_ids,\n",
    "            'Fraud_Prediction': ['Fraudulent' if pred == 1 else 'Legitimate' for pred in predictions],\n",
    "            'Fraud_Probability': fraud_probabilities\n",
    "        })\n",
    "        results['Fraud_Probability'] = results['Fraud_Probability'].round(4)\n",
    "        \n",
    "        os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
    "        json_path = os.path.join(PREDICTIONS_DIR, f'predictions_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json')\n",
    "        excel_path = os.path.join(PREDICTIONS_DIR, f'predictions_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        results.to_json(json_path, orient='records', indent=4)\n",
    "        results.to_excel(excel_path, index=False)\n",
    "        \n",
    "        return results, json_path, excel_path, metadata\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f'Model or artifact files not found: {str(e)}')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f'Fraud prediction failed: {str(e)}')\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage\n",
    "    results, json_path, excel_path, metadata = predict_fraud('data/sample_transactions.csv')\n",
    "    print('Prediction Results:')\n",
    "    print(results.head())\n",
    "    print(f'Results saved to {json_path} and {excel_path}')\n",
    "    print('Metadata:', metadata)\n",
    "'''\n",
    "\n",
    "with open('scripts/predict_fraud.py', 'w') as f:\n",
    "    f.write(predict_fraud_code)\n",
    "\n",
    "print('Prediction script saved to scripts/predict_fraud.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec656c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##CREATE REQUIREMENT FILE\n",
    "requirements = '''\n",
    "streamlit==1.39.0\n",
    "pandas==2.2.2\n",
    "numpy==1.26.4\n",
    "scikit-learn==1.5.2\n",
    "joblib==1.4.2\n",
    "plotly==5.24.1\n",
    "imbalanced-learn==0.12.3\n",
    "matplotlib==3.9.2\n",
    "seaborn==0.13.2\n",
    "chardet==5.2.0\n",
    "'''\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print('Requirements file saved to requirements.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e44f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE .gitignore\n",
    "gitignore = '''\n",
    "# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "\n",
    "# Virtual environments\n",
    "venv/\n",
    "env/\n",
    "\n",
    "# Model artifacts and predictions\n",
    "model_artifacts/\n",
    "predictions/\n",
    "\n",
    "# Logs\n",
    "*.log\n",
    "\n",
    "# Data (to avoid committing sensitive data)\n",
    "data/\n",
    "\n",
    "# OS generated files\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# IDE and editor files\n",
    ".idea/\n",
    "*.sublime-workspace\n",
    "*.sublime-project\n",
    "\n",
    "# Jupyter Notebook checkpoints\n",
    ".ipynb_checkpoints/\n",
    "'''\n",
    "\n",
    "with open('.gitignore', 'w') as f:\n",
    "    f.write(gitignore)\n",
    "\n",
    "print('.gitignore file saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbd461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
